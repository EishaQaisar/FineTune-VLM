# ğŸ–¼ï¸ BLIP2 Image Captioning â€“ Fine-Tuned on Flickr8k

This project **fine-tunes a Vision-Language Model (BLIP2)** on the **Flickr8k dataset** to generate accurate and context-aware captions for images.  
It uses **quantization** and **LoRA (Low-Rank Adaptation)** for efficient training and inference, making it possible to run on limited hardware.

---

## ğŸš€ Features

âœ… **BLIP2 Model Fine-Tuning** â€“ Adapted `blip2-opt-2.7b` for Flickr8k captions.  
âœ… **LoRA + Quantization** â€“ Reduced memory footprint for faster training and inference.  
âœ… **Custom PyTorch Dataset** â€“ Prepares images + captions dynamically for the model.  
âœ… **Efficient Training Loop** â€“ Supports mixed precision training for speed.  
âœ… **Real-Time Inference** â€“ Generates human-like captions for unseen images.  
âœ… **Visualization** â€“ Displays image with predicted caption using `matplotlib`.

---

## ğŸ–¼ Project Screenshot

<p align="center">
  <img src="./images/finetuning-vlm.jpg" alt="IamgeCaptioner" width="600"/>
</p>
---

## ğŸ›  Technologies Used

- **Python** â€“ Core development
- **PyTorch** â€“ Training & fine-tuning
- **Hugging Face Transformers** â€“ BLIP2 Model & Tokenizer
- **BitsAndBytes** â€“ 8-bit quantization for memory efficiency
- **PEFT (LoRA)** â€“ Parameter-efficient fine-tuning
- **Pandas** â€“ Caption dataset processing
- **Matplotlib** â€“ Visualization of results

---

## ğŸ“¦ How to Run

1ï¸âƒ£ Open this project in **Google Colab**  
2ï¸âƒ£ **Upload your `kaggle.json`** file to Colab  
3ï¸âƒ£ Run the cells in order to download the dataset, fine-tune BLIP2, and generate captions  

---
